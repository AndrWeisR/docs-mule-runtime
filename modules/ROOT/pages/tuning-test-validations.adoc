= Performance Test Validations
ifndef::env-site,env-github[]
include::_attributes.adoc[]
endif::[]

To start validating your application's performance, first understand the prerequisites to testing, follow the recommended best practices, and then execute performance testing.


== Prerequisites to Perform Test Validations

Before testing the performance, consider the following points:

* Confirm that your Mule app and its functions work as expected because a wrong flow can give false-positive data.
* Establish the performance criteria for the test cases to determine the goal and scope for tuning. The following questions can help you clarify the performance requirements:
** What are the expected average and peak workloads?
** Is the use case emphasis on throughput, concurrency, response time or size of messages? Some common cases are:
*** Focus on throughput when handling a large volume of transactions is a high priority.
*** Focus on response time or latency if spikes affect user experience and becomes frustrated.
*** Focus on concurrency if it is necessary to support a high number of users connecting at the same time.
*** Focus on managing large messages when the application is transferring, caching, storing, and or processing a payload bigger than 1 MB.
** What is the minimum acceptable throughput?
** What is the maximum acceptable response time?


== Performance Test Validations Best Practices
To apply performance testing consider the following best practices.

=== Use a Controlled Environment
Use a self-controlled environment to obtain repeatable, reliable and consistent data.
To control and stabilize the environment:

* Use an infrastructure made by a triplet:
** Use a dedicated host for the Mule application to have isolated information preventing other running processes from interfering with the results in case of on-premises tests.
** Use a separate and dedicated host for running load client tools that can drive the load without becoming the bottleneck.
** Use a separate and dedicated host to run backend services like database, Tomcat, JMS, ActiveMQ, and so on. Use the same host the application uses in the production environment.
** For on-premises tests, have the load client, runtime server and backend running in the same vpc to decrease network latency.
** For Cloudhub tests, have the load client and backend server running on another dedicated machine but in the same region as the CloudHub worker to decrease network latency.
* Use a wired stable network.
* Don't use your laptop because it may outperform the same application deployed to a customer’s environment, particularly if it is virtualized.

.Performance test infrastructure (Separate host avoid interference)
image::mruntime-tuning-test-infrastructure.png[Performance test infrastructure]

=== Choose the Right Tools

The components used for testing can make a big difference for your use case:

* To get real performance data, choose the most suitable load injector and a backend service configured with the same setup as your customer environment.  +
MuleSoft Performance team uses Apache JMeter; however, there are many enterprise and open source load test tools in the market with different implementations such as single-threaded, multi-threaded, non-blocking, etc. Each one has its advantages and disadvantages depending on the scenario.
* After you configure the tools, continue to use them for consistent comparisons.
* If you change any of the tools, execute all the tests again, and create a new baseline.

=== Isolate the Components of the Test
Because performance is an iterative process and many variables affects the results, it is important that you try to isolate the problem and control these variables. +
To improve the results, split the application into several small ones targeting a specific component or connector. This helps to spot the bottlenecks and direct optimization efforts towards a specific flow element.


=== Use Representative Workloads
The combination of the following practices may affect the performance of your Mule app. +
When you test your application, it is important to imitate real world customer use cases. To obtain customer scenarios, analyze:

* User behavior
* Workload (virtual users, messages, etc)
* Payloads (type, size, complexity)

Additionally, as a best practice:

* Create different scenarios with a repeatable, common, and high load in order to stress Mule, but not too high that pushes Mule to the limit and causes test failure.

* To imitate actual customer behavior, introduce variables as think time between requests and latency to the backend service.




== Execute Performance Test Validations

Before you start the test execution, ideally remove the ramp-up, tear down, and error percentage settings that can affect the throughput calculation results. Considering the best practices from the previous section, execute the testing:

. Apply performance testing to spot and validate the bottlenecks of the applications.
. Use an iterative approach for tuning issues that you find:
.. First, tune Mule’s flows and configurations (only for Cloudhub and on-premises applications)
.. Tune the JVM, GC, and so on. (only for on-premises applications)
.. Finally, tune operating system variables (only for On-premises applications)
. Execute the test several times for trustworthy results. Make one xref:tuning-recommendations.adoc[Tuning Recommendation] change at a time and then retest.
. Check the test results.
