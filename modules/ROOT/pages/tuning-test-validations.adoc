= Performance Test Validations
ifndef::env-site,env-github[]
include::_attributes.adoc[]
endif::[]

To start validating your application's performance, first understand the prerequisites to testing and then follow the recommended best practices.


== Prerequisites to Testing

Consider the following points to test the performance:

* First and most important, confirm that your Mule app and its functions work as expected, because a wrong flow can give false-positive data.
* Establish the performance criteria for the test cases, to determine the goal and scope for tuning. The following questions can help you clarify the performance requirements:
** What are the expected average and peak workloads?
** Is the use case emphasis on throughput, concurrency, response time or size of messages? Some examples of common cases are:
*** Focus on throughput when handling a large volume of transactions is a high priority.
*** Focus on response time or latency if spikes affect user experience and becomes frustrated.
*** Focus on concurrency if it is necessary to support a high number of users connecting at the same time.
*** Focus on managing large messages when the application is transferring, caching, storing, and or processing a payload bigger than 1 MB.
** What is the minimum acceptable throughput?
** What is the maximum acceptable response time?
* Apply performance testing, considering the best practices in the next section, to spot and validate the bottlenecks of the applications. After finding the issues, use an iterative approach for tuning:
** First tune Mule’s flows and configs (Cloudhub and On-premises applications)
** Tune the JVM, GC, and so on. (On-premises applications)
** Finally, operating system variables (On-premises applications)
* Execute the test several times for trustworthy results. Make one change at a time, then retest, and finally check the results.
* Ideally for the throughput calculation, remove the ramp-up, tear-down, and error percentage because this can affect the results.

== Performance Testing Best Practices
Consider the following best practices to start validating the application performance.

=== Use Controlled Environment
Use a self-controlled environment to obtain repeatable, reliable and consistent data.
To control and stabilize the environment, use:

* An infrastructure made by a triplet:
** A dedicated host for the Mule application to have isolated information preventing other running processes from interfering with the results in case of on-premises tests.
** A separate and dedicated host for running load client tools that can drive the load without becoming the bottleneck.
** A separate and dedicated host to run backend services like database, Tomcat, JMS, ActiveMQ, and so on. Use the same host the application uses in the production environment.
** For on-premises tests, have the load client, runtime server and backend running in the same vpc to decrease network latency.
** For Cloudhub tests, have the load client and backend server running on another dedicated machine but in the same region as the CloudHub worker to decrease network latency.
* A wired stable network.
* Don't use your laptop because it may outperform the same application deployed to a customer’s environment, particularly if it is virtualized.
[image here]

== Tools
The components used for testing can make a big difference. Choosing the right load injector and a backend service configured with the same setup as the customer environment can make a considerable difference and can provide real performance data.

The load injector is an important part of the test. Use the general rule of thumb to choose one that is most performant and suitable for the specific use case. The performance team at MuleSoft uses Apache JMeter but there are many enterprise and open source load test tools in the market with different implementations such as single-threaded, multi-threaded, non-blocking, etc. Each one has its advantages and disadvantages depending on the scenario.

After choosing and configuring the tools, stick to it for consistent comparisons, if it is necessary to change them, execute again all the tests, and create a new baseline.

=== Isolate the components of the test
Performance is an iterative process and a lot of variables can affect the results. For this reason, it is important to control as many variables as possible and try to isolate the problem. One of the recommendations is to split the application in several small ones targeting a specific component or connector. This helps to spot the bottlenecks to properly direct the optimization efforts towards a specific flow element and eases the validation process of the results and the improvements.


=== Use representative workloads
When performance is being tested, it is important to mimic real-world customer use cases. That means analyzing the user behavior, workload (virtual users, messages, etc) and payloads (type, size, complexity) to obtain customer scenarios. Make sure to create different scenarios with a repeatable, common, and high load to stress the Mule but not too high that pushes the Mule to the limit and the test fails. To imitate actual customer behavior, introduce variables as think time between requests and latency to the backend service. Keep in mind that the combination of these factors affect the performance of the Mule application.
